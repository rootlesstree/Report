1. Introduction

% Min: too lengthy; consider which sentences you really need.  Get to
% the point early and fast.  Use other (good) summarization papers as
% a model.
  
The volume of scientific literature has grown rapidly. And there is a need to create an automatic 
summarization system which enables researchers to digest knowledge in a short time. In this work, 
we try to use the attributes of scientific document to achieve the goal.

% Min: good.
  Scientific documents have two distinct characteristics. First, its structure must follow certain conventions. That is, you will definitely see the sections arranged like introduction, previous work, method, experiments, discussion and so on. This provides a significant advantage for a summarization system, in being able to leverage this structure. Second, scientific documents will contain a special dimension unlike other texts â€“-- citations. Scientific achievement sare an accumulation of knowledge, partially based on learning from other publications by other members in the community, to make their contribution; therefore, many explanations and criticism will be made by the community through citations.  

The usage of citation sentences makes scientific document summarization an interesting problem to solve. Citation sentences can present certain advanced knowledge processed by human, and generate results that are not available from original article; and how to efficiently use citation sentences to generate summaries is still a open problem. Besides, determining which evaluation method to use is also 


In this work, I will analyze the problem in a more 



2. Related Work


In fact, using citations to generate a summary is not a new idea, as past research have explored in this direction. Based on the problem formulation, it can be catogorized into 2 types. 

In the first type of problem, we assume there is a set of citing sentences and related features provided. The task of this type of problem is figuring out how to select the sentences  

as past research have explored this direction. (S. Teufel, 2005) studies the argumentative zoning for improved citation. (S. Teufel et al., 2006) tries
 to understand the function of citation sentences and utilizes machine learning approach to do automatic classification. 
(V. Qazvinian et al., 2010) summarize by extracting significant key-phrases from the set of citation sentences. 
(V. Qazvinian \& D. Radev, 2008) model the citing sentences in a graph and cluster it with Lexrank method. Besides, 
the effectiveness of citation is verified by  (A. Elkiss et al., 2008) and (Mohammad et al., 2009). Inside both study, 
they use different measures to point out the fact that there are indeed some valuable information in the citation which 
cannot be obtained from the source papers. 

For this type of problem, the assumption of having a set of citing sentences which are well processed might not be realistic. It usually requires lots of labors to annotate the correspodning labels. Besdies, in most of time, we cannot use explicit citation sentence directly. Explicit citation sentence means the sentence containing the citation marker like {} []. As pointed by (), \

and sometimes 



; therefore, how to automatically get good citation sentences become a research topic itself. This type of research topic is not well explored. To best of my knowledge, only () and () have systematic study of this problem; however, again it requires even more human work to get training data.

Due to the limitation of resources I have, in this project, I will focus on the first type of problem, especially those don't require training data.


3. Problem Definition

As discussed above, using citation sentences can be a very 
Here, I try to so.ve the simplest one that 

Given a set of citing sentencs S = {S1,S2,......Sn}, and the length limit of the final sumaary N, we are trying to find out k sentences from S such that words count of these k sentences are smaller than N.





4. Methods

In many AI works, researchers try to build up systems simulating human's behavior. For example, (Kokil) has analyzed the formulation of review papers and tries to figure out how human summarize the scientific article. And based on personal experience, I believe when we are doing summarization, we catogorize sentences into different functional groups as topics, methods, experiments, results, ...etc. And we will consider the best combination between the groups and select appropriate sentences correspondingly to form the final summary. As we can observe, formulating different groups is the most important step in the whole process. Human might use functional or semantic meanings of sentence to classify. And in this work, I want to simulate this process and try to find out the useful representations which can generate a good summary.

As mentioned above, (C-LEXRANK) will classify citing sentences into groups by cosine tf-idf similaries between the sentences. This step is similar to human behavior in the sense that it will produce different groups; therefore, it's a good start for trying to simulate human decision behavior. However, tf-idf is a statistical method which contains no significal NLP meaning. In this work, I try to tackle the problem on top of C-Lexrank, and replace the similarity measure with more semantical methods. 




5. Evaluation




5.1 Data

The data we use to evalute is collected from ACL Anthology. We randomly sample 10 articles from the Anthology. Then we need to check if the sampled articles have enough citations. We replace the ones having less than 10 citations with the new sampled articles. And repeat this procedure till all 10 articles have enough citations. The final selection of the the data is listed as follows:  P99-1026, W11-2821, W06-3312,  P07-3014,  C08-1122,  P10-1024, C00-1073, N06-1031, J81-3002, J93-1005. The number inside the parantheses is the citing sentences used. Due to the citation processing problem mentioned above, currently the citation sentences are collected by human work, and the reference scope and explicit citation are also identified by myself.    

5.2 ROUGE score and golden standard summary


In fact, trying to objectively evalute a subjective task is very difficult. There are many exisitng evalutaions there but none of them is perfect. In this project, we are going to use ROUGE to evaluate the result. ROUGE stands for Recall-Oriented Unders study for Gisting Evaluation. It counts the number of overlapping units such as n-gram, word sequences and word pairs between the system-generated summary and ideal summaries created by humans; however, it won't give high score to the summary with synonyms of words appearing in the golden summary. In fact, in (C-LEXRANK), they notice this problem and use pyrimad method to evalutate the result. pyrimad method requires lots of annotation works to identify certain useful words, called nuggest, and try to calculate the coverage of nuggests in the summary. It prefers results using less words to express condensed gists. Indeed, it's hard to find a perfect measure which takes everything into consideration. Due to the limitation of resources, I believe ROUGE is the best measure we should use as it requires only human generated summary and contains less annotation works. In this project, we have 6 group members and everybody will summarize all 10 sampled papers. To make the summarization easier and suitable for other tasks, we summarize the article by extrating whole sentences from the source paper, and the length of the summary is limited around 100 words. It roughly contains 4-5 sentences.



5.3 Result and Discussion



6. Conclusion
