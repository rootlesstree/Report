\documentclass[hyp]{socreport}

\begin{document}
\pagenumbering{roman}
\title{Scientific Document Summarization Exploiting Citing Documents}
\author{Patrick Chen}
\projyear{2014}
\projnumber{0}
\advisor{0}
\deliverables{
    \item Report: 1 Volume
}
\maketitle

\begin{abstract}
Abstract eventually goes here.
\end{abstract}

\section{Introduction}

% Min: too lengthy; consider which sentences you really need.  Get to
% the point early and fast.  Use other (good) summarization papers as
% a model.
  The volume of scientific literature has grown rapidly. And there is a need to create an automatic 
summarization system which enables researchers to digest knowledge in a short time. In fact, Summarization 
is not a new field in NLP community; however, lots of previous work are done in the data like news, and how 
to summarize scientific documents is still under discovering.

% Min: good.
  Scientific documents have two distinct characteristics. First, its structure must follow certain conventions. 
That is, you will definitely see the sections arranged like introduction, previous work, method, experiments, 
discussion and so on. This provides a significant advantage for a summarization system, in being able to leverage this 
structure. Second, scientific documents will contain a special dimension unlike other texts –-- citations. Scientific achievements 
are an accumulation of knowledge, partially based on learning from other publications by other members in the community, to make their 
contribution; therefore, many explanations and criticism will be made by the community through citations.  

  The second attribute makes scientific document summarization extremely special and attractive to me.
% Min: not clear what you mean by this sentence.  Need to paint your work as important and different.
Many 
previous works solve it by either natural language or information retrieval methods and it produces promising 
results. So in this project, I will focus on using citations to make a good summary.   

\section{Summarization using Citing Sentences}

% Min: be more complete in your description of past research.
% Ideally, you should have at least 1 to 2 sentences, With detailed
% technical information, For each article that you're citing.  Try to
% structure it in a way that presents the weaknesses of their
% research, which will in turn motivate your research/approach.
  Using citations to generate a summary is not a new idea, as past research have explored 
this direction. (S. Teufel, 2005) studies the argumentative zoning for improved citation. (S. Teufel et al., 2006) tries
 to understand the function of citation sentences and utilizes machine learning approach to do automatic classification. 
(V. Qazvinian et al., 2010) summarize by extracting significant key-phrases from the set of citation sentences. 
(V. Qazvinian \& D. Radev, 2008) model the citing sentences in a graph and cluster it with Lexrank method. Besides, 
the effectiveness of citation is verified by  (A. Elkiss et al., 2008) and (Mohammad et al., 2009). Inside both study, 
they use different measures to point out the fact that there are indeed some valuable information in the citation which 
cannot be obtained from the source papers. 

% Min: this is too nonspecific.  Also, it should be moved to the
% next paragraph where you start to talk about your methods. So I've done that.
%% In this project, we are going to create our own system which tries to improve 
%% and combine some methods mentioned above. And the details of the system will be discussed in the following section. 

\section{System Description}

In this project, we are going to create our own system which tries to improve 
and combine some methods mentioned above.

\subsection{General System Requirements}

% Min: try not to preface sentences with meaningless navigation
% phrases like ``in this section'', unless the section is long
% enough(E.g., at least 3 to 5 pages long).
  In this section, I am going to discuss the basic functions that the baseline system should have, and in the next section, 
my baseline system will be explained based on the structure introduced in this section. In general, a complete summary 
is generated using the following pipeline:

% Min: if this part is just an overview, some of the following comments
% would be better addressed in the detailed sections

% Min: somehow, This section reads much more about processing the
% citing sentences than about the summarization algorithm. It does not
% read balanced.  You may want to contact some of the descriptions on
% processing the siting sentences together into a single step, so as
% to lessen the imbalance.

% Min: this might be better turned as ``extraction'', instead of ``parsing''.
(a) Parsing source paper: Parsing the source paper to get some important information such as title and authors of the paper,
or anything that might be useful in the later stages. Save these data and proceed to the next step.

(b) Find out all citing papers: In order to obtain the citing sentences, we must collect all the papers which cite the source 
paper first. 
% Min: this is a claim. You need to present evidence to back this
% up. Give a citation, or an example for which this is valid.
One thing to note is that there is no way to verify the total number of citing papers. We may use some tools or 
databases to find out citing papers as more as possible, but it simply cannot promise the completeness. After getting the 
citing papers, the system is prepared to extract information from it.

% Min: define and talk more about implicit citations, and how they are different from explicit ones. 
(c) Extracting citing sentences: Inside each citing paper, there must be either explicit or implicit citation. Usually the 
citation will be written as author plus year inside the parentheses or brackets as [Authors, Year], sometimes it might just 
use a number and we need to go through the reference section in the paper to know the work being cited. For the explicit citation, 
citing sentences are just next to these indicators so you can easily find out the correct citing sentences; however, sometimes the 
citing sentences are not close to the indicators, or beside the closest sentence, adjacent sentences contain more important information, 
and it’s called implicit citation. The system should specify what kinds of citation it is going to extract and the methods to detect 
the citing sentences.

(d) Using citing sentences to generate the summary: The most important step in the system is generating the summary. After getting 
all available citing sentences, we will be able to use them to summary the source paper.  

(e) Evaluate the summary: After generating the summary, we need to evaluate its quality. There are many existing methods 
to evaluate the result. It will be discussed in details in the section 4.2. 

\subsubsection{Baseline system}

In this section, I will describe the details of my baseline system by explaining each component introduced in the last section. 

% Min: you should cite ParsCit's paper.  There is a separate paper for
% SectLabel, which labels document sections.  You probably use ParsCit
% for citing sentence identification, but use SectLabel for parsing
% the actual citing document.

% Min: in the final version of your report, you will need to emphasize
% just a scientific contribution of your work, and not the technical
% difficulties encountered. Information about specific libraries and
% technical problem should not be in the main part of your thesis; you
% should mention them but in an appendix instead.
(a) Parsing source paper: in the baseline system, all the information needed is author and title, and this task will be handled by 
the ParsCit library. ParsCit is a friendly tool to extract some useful information for scientific documents. The only inconveniency 
is that it requires the input format to be the XML, especially the XML file generated by the OmniPage commercial software. But generally 
the scientific document is easily found in pdf format; therefore, all the papers need to be passed into OmniPage first before extracting 
the author and title of the paper.

(b) Find out all citing papers: the simplest way to track the citations of certain paper is using Google Scholar Search Engine. Although 
it might not be complete, it provides a satisfactory result. Originally, I planned to use a library developed by WING group at NUS to 
automatically download the query results from Google Scholar; however, because there are some version problems, it does not function properly 
right now; therefore, I decide to manually download all the pdf files from Google at this stage. Also, because I believe this stage should not 
cost much time, if the download link requires me to login from NUS library, I will simply discard the paper.

(c) Extracting citing sentences: After getting the citing papers’ pdf files, I will run the OmniPage in batch to convert pdf to XML, and then 
again using ParsCit to get the citing sentences. Basically, ParsCit will return you whole context of citation. That is, it will include 3 more 
sentences before and after the sentence where citation indicator shows. In the baseline system, I will simply extract just the sentence containing 
citation indicator and omit all the others first. Although (A. Abu-Jbara \& D. Radev, 2012) has shown that there might be useful information in the 
surrounding sentences, in the first step, it is still better for me to extract single sentence only, or to extract sentences by human.

% Min: you need to properly introduce C-Lexrank in the related work section earlier, and give the technical details for how it works there.
(d) Using citing sentences to generate the summary: in this baseline system, I am going to use C-Lexrank (V. Qazvinian \& D. Radev, 2008) as the 
method to summarize. The spirit behind C-Lexrank is to classify citing sentences into different clusters by calculating cosine similarity. And it 
subsequently calculates LexRank within each cluster to find the most salient sentences of each cluster. More information can be found in the original paper. 

(e) Evaluate the summary: in the original paper, C-Lexrank is evaluated by the Pyramid method. The spirits of C-Lexrank is to use minimal 
sentences to describe as more content as possible; therefore, it directly meets the idea behind the pyramid method which calculates the 
% Min: same as above. You need to introduce what is an SCU.
percent of coverage of most important Summarization Content Units (SCUs). So it is reasonable for the original paper to use pyramid method 
to evaluate the C-Lexrank; however, there are two problems. First, this project is going to deliver a summary in the end, instead of only
valuable sentences. 
% Min: be more specific about the shortcomings, and what adaptations you need to make.

% Min: this section reads backwards. You describe the work as you did
% it, ibut not in a way that helps the reader understand.  You should
% describe your actual evaluation method first (because it's most
% important), and then describe alternatives which you have tried and
% decided not to adopt.  Important things come first.
Apparently C-Lexrank needs some light alternations to meet this requirement. Second, the Pyramid method is a method which 
requires lots of human work. Humans need to manually annotate SCUs for both standard and system-generated summarizes. For this project, we do not 
have enough resources to do so; besides, if we need to tune or optimize the system in the future, we need an automatic evaluation method to 
achieve it; therefore, in this project, we will not use Pyramid method to do the evaluation. On the other hand, ROUGE (Lin, 2004) has been 
proposed to use only few human summaries to judge the quality of summarization. It calculates recall, precision and F-measures by different 
units like N-grams or LCS. Although it might be unfair to directly apply ROUGE to C-Lexrank method directly in this stage, considering the 
resources we have, it is the best method we can have to evaluate the results.

\section{Experiment}

% Min: try not to have empty section headers.
\subsection{Data Preparation}
  The test data is obtained by randomly sampling 10 papers from ACL anthology, namely P99-1026, W10-1919, S12-1032, P07-3014, C08-1122,
P10-1024, W93-0225, Y09-2051, D12-1074, O90-1002. I use their citing sentences to generate the summary; however, only 
C08-1122 (13 citations), P10-1024 (10), P99-1026 (24) and W93-0225 (6) have enough (at least 5) citing sentences; therefore, the experiment was only be done on these 4 papers.

\subsection{Evaluation}
  We are going to generate a summary containing 4-5 sentences and around 120-150 words. 
% Min: this details in the wrong place. Please move it earlier, to where you're introducing the evaluation method.
I will use ROUGE as the evaluation method as mentioned above. ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation.
It counts the number of overlapping units such 
as n-gram, word sequences and word pairs between the system-generated summary and ideal summaries created by humans.
% Min: describe the demographics of the evaluation team.  Number, age, description, and agreement levels are useful.
In this project, 
the gold summaries are generated by group members. We will do the extraction summarization on source paper to generate summaries 
based on the sentences in the source paper, and there are 2 for P10-1024 and C08-1122, 1 for W93-0225 and P99-1026 now.

\section{Results and Discussion}

% Min: this detail about ROUGE/C-Lexrank should come earlier as well.
% In the results section, you should only give (empirical) results of
% the experiments, And offer some description of them.  Interpret the
% results later in the discussion part.  Details about how you carry
% out the experiment should come before, in the main evaluation
% section.
  Due to the fact that C-LexRank originally is not evaluated by ROUGE, it might be difficult to compare to other works; 
nevertheless, I find out in (Mohammad et al., 2009), they use ROUGE-2 to evaluate various methods including C-LexRank; 
and in (Mei and Zhai 2008), they use ROUGE-1 and ROUGE-L on random selection and other popular methods as MEAD. It might 
be unfair to compare to their values directly, but it is good to refer their results. So I will use them as a comparison 
baseline to point out what's the reasonable values C-LexRank should give. The evaluation result of my C-LexRank system is 
shown as follows:

% Min: these actual results should be pushed into an appendix.  The
% summarized results that you present later are better for the main body
% of your report.
P10-1024
---------------------------------------------

3 ROUGE-1 Average\_R: 0.32819 (95\%-conf.int. 0.32819 - 0.32819)

3 ROUGE-1 Average\_P: 0.30797 (95\%-conf.int. 0.30797 - 0.30797)

3 ROUGE-1 Average\_F: 0.31776 (95\%-conf.int. 0.31776 - 0.31776)

---------------------------------------------

3 ROUGE-2 Average\_R: 0.04669 (95\%-conf.int. 0.04669 - 0.04669)

3 ROUGE-2 Average\_P: 0.04380 (95\%-conf.int. 0.04380 - 0.04380)

3 ROUGE-2 Average\_F: 0.04520 (95\%-conf.int. 0.04520 - 0.04520)

---------------------------------------------

3 ROUGE-L Average\_R: 0.30116 (95\%-conf.int. 0.30116 - 0.30116)

3 ROUGE-L Average\_P: 0.28261 (95\%-conf.int. 0.28261 - 0.28261)

3 ROUGE-L Average\_F: 0.29159 (95\%-conf.int. 0.29159 - 0.29159)



P99-1026
---------------------------------------------

1 ROUGE-1 Average\_R: 0.45763 (95\%-conf.int. 0.45763 - 0.45763)

1 ROUGE-1 Average\_P: 0.39130 (95\%-conf.int. 0.39130 - 0.39130)

1 ROUGE-1 Average\_F: 0.42187 (95\%-conf.int. 0.42187 - 0.42187)

---------------------------------------------

1 ROUGE-2 Average\_R: 0.10256 (95\%-conf.int. 0.10256 - 0.10256)

1 ROUGE-2 Average\_P: 0.08759 (95\%-conf.int. 0.08759 - 0.08759)

1 ROUGE-2 Average\_F: 0.09449 (95\%-conf.int. 0.09449 - 0.09449)

---------------------------------------------

1 ROUGE-L Average\_R: 0.42373 (95\%-conf.int. 0.42373 - 0.42373)

1 ROUGE-L Average\_P: 0.36232 (95\%-conf.int. 0.36232 - 0.36232)

1 ROUGE-L Average\_F: 0.39063 (95\%-conf.int. 0.39063 - 0.39063)



C08-1122
---------------------------------------------

1 ROUGE-1 Average\_R: 0.25498 (95\%-conf.int. 0.25498 - 0.25498)

1 ROUGE-1 Average\_P: 0.22069 (95\%-conf.int. 0.22069 - 0.22069)

1 ROUGE-1 Average\_F: 0.23660 (95\%-conf.int. 0.23660 - 0.23660)

---------------------------------------------

1 ROUGE-2 Average\_R: 0.05221 (95\%-conf.int. 0.05221 - 0.05221)

1 ROUGE-2 Average\_P: 0.04514 (95\%-conf.int. 0.04514 - 0.04514)

1 ROUGE-2 Average\_F: 0.04842 (95\%-conf.int. 0.04842 - 0.04842)

---------------------------------------------

1 ROUGE-L Average\_R: 0.23108 (95\%-conf.int. 0.23108 - 0.23108)

1 ROUGE-L Average\_P: 0.20000 (95\%-conf.int. 0.20000 - 0.20000)

1 ROUGE-L Average\_F: 0.21442 (95\%-conf.int. 0.21442 - 0.21442)



W93-0225
---------------------------------------------

4 ROUGE-1 Average\_R: 0.36842 (95\%-conf.int. 0.36842 - 0.36842)

4 ROUGE-1 Average\_P: 0.36601 (95\%-conf.int. 0.36601 - 0.36601)

4 ROUGE-1 Average\_F: 0.36721 (95\%-conf.int. 0.36721 - 0.36721)

---------------------------------------------

4 ROUGE-2 Average\_R: 0.05298 (95\%-conf.int. 0.05298 - 0.05298)

4 ROUGE-2 Average\_P: 0.05263 (95\%-conf.int. 0.05263 - 0.05263)

4 ROUGE-2 Average\_F: 0.05280 (95\%-conf.int. 0.05280 - 0.05280)

---------------------------------------------

4 ROUGE-L Average\_R: 0.30921 (95\%-conf.int. 0.30921 - 0.30921)

4 ROUGE-L Average\_P: 0.30719 (95\%-conf.int. 0.30719 - 0.30719)

4 ROUGE-L Average\_F: 0.30820 (95\%-conf.int. 0.30820 – 0.30820)


I summarize the results as follow(use F score):

           C08-1122     W93-0225     P10-1024     P99-1026
ROUGE-1     0.237        0.367        0.318        0.422    
ROUGE-2     0.048        0.053        0.040        0.088
ROUGE-L     0.214        0.308        0.292        0.391

And the results combined both (Mohammad et al., 2009) and (Mei and
Zhai 2008) are shown as follows:

                    Random     LEAD     MEAD     C-LexRank
ROUGE-1            0.230      0.301      0.401        X
ROUGE-2            0.10          X       X           0.13
ROUGE-L            0.214      0.292      0.362        X


the results generated from my system vary slightly. I believe it is because of the number of citing sentences, and the 
quality of the summarization. For example, P99-1026 gives a good result and it utilizes 26 citing sentences. And for W93-0225, 
although it only contains 7 citing sentences, all 7 sentences give a concrete and focused illustration of work of W93-0225, 
so the result is better than C08-1122, whose citing sentences contain less valuable information. 

% Min: you'll need to eventually show statistically significant
% improvement in your system's performance.  However, this is fine for
% now.
  And comparing to other methods, my baseline system generally outperforms the random and LEAD method, and performs equally 
to MEAD, which reflects that it's value is reasonable. About the C-Lexrank in ROUGE-2, there is a significant gap between 
my result and (Mohammad et al., 2009). I guess it is because their experiments are focused on Dependency Parsing papers, 
and citing sentences will contain more information in that kind of papers.  

    But comparing to method developed in (Mei and Zhai 2008) which achieves 0.467 in ROUGE-1 and 0.444 in ROUGE-L, it seems 
like my baseline system is not good enough yet. It also reflects the characteristics that C-LexRank is trying to enhance the 
diversity of citations within minimal length, but it does not consider the coherence or other factors which will affect the 
quality of summary. 

\section{Conclusion}

In this project, I build a summarization system based on the C-Lexrank method. It will extract citing sentences from the pdf 
documents, and automatically compose a summary. The experiments show a reasonable result and in the meantime reflect characteristics
 of C-Lexrank method. To further improve the result, I will work on finding functions of sentences inside each C-Lexrank cluster and 
try to map citing sentences to a template in the future.

\bibliographystyle{socreport}
\bibliography{report_v0}
\end{document}

% Min: try to reformat into bib format.  I've placed an example in your repository.
% To move into report_v0.bib
References

Amjad Abu-Jbara, Dragomir Radev. 2012. Reference Scope Identification in citing Sentences In Conference of the North American Chapter 
of the Association for Computational Linguistics.

Aaron Elkiss, siwei Shen, Anthony Fader, Gunes Erkan, David States, and Dragomir Radev. 2008. Blind Men and Elephants: What Do Citation Summaries 
Tell Us About a Research Article? In Journal of the American Society for Information Science and Technology.

Chin-Yew Lin. 2004. ROUGE:a Package for Automatic Evaluation of Summaries. In Proceedings of the Workshop on Text summarization Btanches Out(WAS 2004).

Qiaozhu Mei and ChengXiang Zhai. 2008. Generating Impact-Based summaries for Scientific Literature. In the Proceedings of ACL-08: HLT.

Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed Hassan, Pradeep Muthukrishan, Vahed Qazvinian, Dragomir Radev, David Zajic. 2009. 
Using Citations to Generate surveys of Scientific Paradigms. In 2009 Annual Conference of the North American Chaper of the ACL.

Vahed Qazvinian and Dragomir R. Radev. 2008. Scientific Paper Summarization Using Citation Summary Networks. In International Condference on Computational Linguistics.  

Vahed Qazvinian, Dragomir R. Radev, Arzucan Ozgur. 2010. Citation Summarization Through Keyphrase Extraction. In Proceedings of 
the 23rd International Conference on Computational Linguistics.

Simone Teufel. 2005. Argumentative Zoning for improved citation indexing. In "Computing Attitude and Affect in Text: Theory and Applications" 
James G. Shanahan, Yan Qu, Janyce Wiebe (Eds.) Springer, Dordrecht, The Netherlands, 2005.

Simone Teufel, Advaith Siddharthan, Dan Tidhar. 2006. Automatic classification of citation function. In Proceedings of EMNLP-06.




